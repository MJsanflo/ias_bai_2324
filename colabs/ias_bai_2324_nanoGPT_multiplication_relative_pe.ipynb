{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "lQLFBSB7tnyk",
        "piBx7LTZtRb2",
        "yjGOAXu8tarm",
        "5_vxGIHs0EXe",
        "gEI_ov8B0QAS",
        "t5UUlwkQ27Du",
        "hgo4Ffvf0tGI",
        "3r0kvdIfuCu-",
        "OIVMe9JDuG5f"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJsanflo/ias_bai_2324/blob/main/ias_bai_2324_nanoGPT_multiplication_relative_pe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluating transformer for multipliatons"
      ],
      "metadata": {
        "id": "3Cuv8KkFs5hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "GMO5SI9vsv4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup google drive folder\n",
        "\n",
        "You might have to create a folder named bai where the path points to.\n",
        "Then create a folder named data/multiplicaton and a folder named out-multi."
      ],
      "metadata": {
        "id": "lQLFBSB7tnyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = \"/content/drive/MyDrive/bai/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwsGtbKizu5_",
        "outputId": "6a44a8b1-16b9-4b6d-95b3-83e447994f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "piBx7LTZtRb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy datasets tiktoken wandb tqdm transformers matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8axmeFOcEXAN",
        "outputId": "b35c4728-7fa1-4ce4-934c-5da14cdea1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, dill, tiktoken, multiprocess, gitdb, GitPython, wandb, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.41 datasets-2.16.1 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.11 multiprocess-0.70.15 sentry-sdk-1.39.2 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.5.2 wandb-0.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log into wandb"
      ],
      "metadata": {
        "id": "yjGOAXu8tarm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yOYewsuEslk",
        "outputId": "c41e3110-435a-4940-9c80-9428d5c9d3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "Aborted!\n",
            "Exception ignored in atexit callback: <function _Manager._atexit_setup.<locals>.<lambda> at 0x7e52593d3f40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 156, in <lambda>\n",
            "    self._atexit_lambda = lambda: self._atexit_teardown()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 165, in _atexit_teardown\n",
            "    self._teardown(exit_code)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_manager.py\", line 176, in _teardown\n",
            "    result = self._service.join()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/service/service.py\", line 250, in join\n",
            "    ret = self._internal_proc.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check gpu config"
      ],
      "metadata": {
        "id": "zTsES-HwN-YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run when using GPU\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmDHkM-8TMOd",
        "outputId": "56cb2b4e-9934-4567-fcb4-808bba0895ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set global variables as config"
      ],
      "metadata": {
        "id": "1bzpCoI80JHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = \"out-multi\"\n",
        "eval_interval = 250  # keep frequent because we'll overfit\n",
        "eval_iters = 200\n",
        "log_interval = 50  # don't print too too often\n",
        "\n",
        "# we expect to overfit on this small dataset, so only save when val improves\n",
        "always_save_checkpoint = False\n",
        "\n",
        "wandb_log = True\n",
        "wandb_project = \"multiplication-char\"\n",
        "wandb_run_name = \"relative_pos_enc_100k\"  # 'run' + str(time.time())\n",
        "init_from = \"resume\"  # change to resume if you have a model already\n",
        "\n",
        "dataset = \"multiplication\"\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256  # context of up to 256 previous characters\n",
        "\n",
        "# baby GPT model :)\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.0\n",
        "\n",
        "pos_enc = \"relative\"  # absolute or relative\n",
        "learning_rate = 1e-3  # with baby networks can afford to go a bit higher\n",
        "max_iters = 100000\n",
        "lr_decay_iters = 100000  # make equal to max_iters usually\n",
        "min_lr = 1e-4  # learning_rate / 10 usually\n",
        "beta2 = 0.99  # make a bit bigger because number of tokens per iter is small\n",
        "warmup_iters = 100  # not super necessary potentially\n",
        "\n",
        "device = \"cuda\"  # run on gpu\n",
        "compile = True  # do  torch compile the model\n"
      ],
      "metadata": {
        "id": "AO7ql85Hucxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import all dependencies"
      ],
      "metadata": {
        "id": "5_vxGIHs0EXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "import tiktoken\n",
        "import time\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from contextlib import nullcontext\n",
        "from random import randint\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "avvHto9Nz5DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "huTDrFfVt-41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate files for training and evaluation\n",
        "\n",
        "If these files exist on the dive than you dont have to rerun these"
      ],
      "metadata": {
        "id": "gEI_ov8B0QAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "max_num_of_digits = 3\n",
        "min_num_of_digits = 1\n",
        "\n",
        "def prepare(input_file_path: str):\n",
        "    with open(input_file_path, \"r\") as f:\n",
        "        data = f.read()\n",
        "    print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "    # get all the unique characters that occur in this text\n",
        "    chars = sorted(list(set(data)))\n",
        "    vocab_size = len(chars)\n",
        "    print(\"all the unique characters:\", \"\".join(chars))\n",
        "    print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "    # create a mapping from characters to integers\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    def encode(s):\n",
        "        return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "\n",
        "    def decode(l):\n",
        "        return \"\".join(\n",
        "            [itos[i] for i in l]\n",
        "        )  # decoder: take a list of integers, output a string\n",
        "\n",
        "    # create the train and test splits\n",
        "    n = len(data)\n",
        "    train_data = data[: int(n * 0.9)]\n",
        "    val_data = data[int(n * 0.9) :]\n",
        "\n",
        "    # encode both to integers\n",
        "    train_ids = encode(train_data)\n",
        "    val_ids = encode(val_data)\n",
        "    print(f\"train has {len(train_ids):,} tokens\")\n",
        "    print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "    # export to bin files\n",
        "    train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "    val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "    train_ids.tofile(os.path.join(data_folder, dataset, \"train.bin\"))\n",
        "    val_ids.tofile(os.path.join(data_folder, dataset, \"val.bin\"))\n",
        "\n",
        "    # save the meta information as well, to help us encode/decode later\n",
        "    meta = {\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"itos\": itos,\n",
        "        \"stoi\": stoi,\n",
        "    }\n",
        "    with open(os.path.join(data_folder, dataset, \"meta.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(meta, f)\n",
        "\n",
        "\n",
        "def generate_numbers(filename: str):\n",
        "    print(\n",
        "        f\"Generating all {max_num_of_digits} x {max_num_of_digits} digits down to {min_num_of_digits} x {min_num_of_digits}\"\n",
        "    )\n",
        "    number_list = []\n",
        "    for length in range(max_num_of_digits):\n",
        "        for i in range(10**length, (10 ** (length + 1)) - 1):\n",
        "            for j in range(10**length, (10 ** (length + 1)) - 1):\n",
        "                number_list.append(f\"What is {i} times {j}?\\n{i*j}\\n\\n\")\n",
        "\n",
        "    random.shuffle(number_list)\n",
        "    with open(f\"{filename}.txt\", \"w\") as file:\n",
        "        for number in number_list[20000:]:\n",
        "            file.write(number)\n",
        "\n",
        "    with open(f\"{filename}_test.txt\", \"w\") as file:\n",
        "        for number in number_list[0:20000]:\n",
        "            file.write(number)\n",
        "\n",
        "\n",
        "def generateOutOfDistributionNumbers(firstDigitLength, secondDigitLength, amount=10000):\n",
        "    # generate out of distribution numbers (4x3)\n",
        "    print(\"Generating out of distribution numbers\")\n",
        "    with open(os.path.join(data_folder, dataset, \"ood_numbers.txt\"), \"w\") as file:\n",
        "        for _ in range(amount):\n",
        "            first_number = randint(\n",
        "                10 ** (firstDigitLength - 1), 10**firstDigitLength - 1\n",
        "            )\n",
        "            second_number = randint(\n",
        "                10 ** (secondDigitLength - 1), 10**secondDigitLength - 1\n",
        "            )\n",
        "            file.write(\n",
        "                f\"What is {first_number} times {second_number}?\\n{first_number* second_number}\\n\\n\"\n",
        "            )\n",
        "\n",
        "data_folder = drive_path + \"data\"\n",
        "# generate files\n",
        "filename = \"numbers\"\n",
        "path = os.path.join(data_folder, dataset, filename)\n",
        "generate_numbers(path)\n",
        "prepare(path + \".txt\")\n",
        "generateOutOfDistributionNumbers(4, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4nMmMr3ztyR",
        "outputId": "38cbe7d5-79af-41db-81f9-3a2c260d085d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating all 3 x 3 digits down to 1 x 1\n",
            "length of dataset in characters: 24,511,242\n",
            "all the unique characters: \n",
            " 0123456789?Waehimst\n",
            "vocab size: 21\n",
            "train has 22,060,117 tokens\n",
            "val has 2,451,125 tokens\n",
            "Generating out of distribution numbers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "t5UUlwkQ27Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Full definition of a GPT Language Model, all of it in this single file.\n",
        "References:\n",
        "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "2) huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "\"\"\"\n",
        "\n",
        "class RelativePosition(nn.Module):\n",
        "    def __init__(self, num_units, max_relative_position):\n",
        "        super().__init__()\n",
        "        self.num_units = num_units\n",
        "        self.max_relative_position = max_relative_position\n",
        "        self.embeddings_table = nn.Parameter(\n",
        "            torch.Tensor(max_relative_position * 2 + 1, num_units)\n",
        "        )\n",
        "        nn.init.xavier_uniform_(self.embeddings_table)\n",
        "\n",
        "    def forward(self, length_q, length_k):\n",
        "        range_vec_q = torch.arange(length_q)\n",
        "        range_vec_k = torch.arange(length_k)\n",
        "        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
        "        distance_mat_clipped = torch.clamp(\n",
        "            distance_mat, -self.max_relative_position, self.max_relative_position\n",
        "        )\n",
        "        final_mat = distance_mat_clipped + self.max_relative_position\n",
        "        final_mat = torch.Tensor(final_mat)\n",
        "        embeddings = self.embeddings_table[final_mat]\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.pos_enc = config.pos_enc\n",
        "        print(f\"Using {config.pos_enc} positional encoding!\")\n",
        "        # new part for relative positional encoding\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.max_relative_position = 10\n",
        "        self.relative_position_k = RelativePosition(\n",
        "            self.head_dim, self.max_relative_position\n",
        "        )\n",
        "        self.relative_position_v = RelativePosition(\n",
        "            self.head_dim, self.max_relative_position\n",
        "        )\n",
        "\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
        "        if not self.flash or self.pos_enc == \"relative\":\n",
        "            print(\n",
        "                \"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\"\n",
        "            )\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                    1, 1, config.block_size, config.block_size\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        def relative_encoding(x):\n",
        "            # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "            B, T, C = x.shape\n",
        "            query, key, value = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "\n",
        "            r_q1 = query.view(B, -1, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
        "            r_k1 = key.view(B, -1, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
        "            attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))\n",
        "\n",
        "            r_q2 = (\n",
        "                query.permute(1, 0, 2)\n",
        "                .contiguous()\n",
        "                .view(T, B * self.n_head, self.head_dim)\n",
        "            )\n",
        "            r_k2 = self.relative_position_k(T, T)\n",
        "            attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)\n",
        "            attn2 = attn2.contiguous().view(B, self.n_head, T, T)\n",
        "            attn = (attn1 + attn2) / math.sqrt(key.size(-1))\n",
        "\n",
        "            attn = attn.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "            attn = self.attn_dropout(attn)\n",
        "\n",
        "            # attn = [batch size, n heads, query len, key len]\n",
        "            r_v1 = value.view(B, -1, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
        "            weight1 = torch.matmul(attn, r_v1)\n",
        "            r_v2 = self.relative_position_v(T, T)\n",
        "            weight2 = attn.permute(2, 0, 1, 3).contiguous().view(T, B * self.n_head, T)\n",
        "            weight2 = torch.matmul(weight2, r_v2)\n",
        "            weight2 = (\n",
        "                weight2.transpose(0, 1)\n",
        "                .contiguous()\n",
        "                .view(B, self.n_head, T, self.head_dim)\n",
        "            )\n",
        "\n",
        "            x = weight1 + weight2\n",
        "\n",
        "            # x = [batch size, n heads, query len, head dim]\n",
        "\n",
        "            x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "            # x = [batch size, query len, n heads, head dim]\n",
        "\n",
        "            x = x.view(B, -1, self.n_embd)\n",
        "\n",
        "            # x = [batch size, query len, hid dim]\n",
        "\n",
        "            # output projection\n",
        "            y = self.resid_dropout(self.c_proj(x))\n",
        "\n",
        "            # y = [batch size, query len, hid dim]\n",
        "\n",
        "            return y\n",
        "\n",
        "        def absolute_encoding(x):\n",
        "            # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "            B, T, C = x.shape\n",
        "            # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "            q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "            k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "                1, 2\n",
        "            )  # (B, nh, T, hs)\n",
        "            q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "                1, 2\n",
        "            )  # (B, nh, T, hs)\n",
        "            v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "                1, 2\n",
        "            )  # (B, nh, T, hs)\n",
        "\n",
        "            # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "            if self.flash:\n",
        "                # efficient attention using Flash Attention CUDA kernels\n",
        "                y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                    q,\n",
        "                    k,\n",
        "                    v,\n",
        "                    attn_mask=None,\n",
        "                    dropout_p=self.dropout if self.training else 0,\n",
        "                    is_causal=True,\n",
        "                )\n",
        "            else:\n",
        "                # manual implementation of attention\n",
        "                att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "                att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "                att = F.softmax(att, dim=-1)\n",
        "                att = self.attn_dropout(att)\n",
        "                y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "            y = (\n",
        "                y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "            )  # re-assemble all head outputs side by side\n",
        "\n",
        "            # output projection\n",
        "            y = self.resid_dropout(self.c_proj(y))\n",
        "            return y\n",
        "\n",
        "        if self.pos_enc == \"relative\":\n",
        "            return relative_encoding(x)\n",
        "        else:\n",
        "            return absolute_encoding(x)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    pos_enc: str = \"relative\"\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "                drop=nn.Dropout(config.dropout),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f=LayerNorm(config.n_embd, bias=config.bias),\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = (\n",
        "            self.lm_head.weight\n",
        "        )  # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"c_proj.weight\"):\n",
        "                torch.nn.init.normal_(\n",
        "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
        "                )\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert (\n",
        "            t <= self.config.block_size\n",
        "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
        "            )\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(\n",
        "                x[:, [-1], :]\n",
        "            )  # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(\n",
        "            self.transformer.wpe.weight[:block_size]\n",
        "        )\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, \"bias\"):\n",
        "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
        "        override_args = override_args or {}  # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == \"dropout\" for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
        "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
        "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args[\"block_size\"] = 1024  # always 1024 for GPT model checkpoints\n",
        "        config_args[\"bias\"] = True  # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if \"dropout\" in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args[\"dropout\"] = override_args[\"dropout\"]\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [\n",
        "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
        "        ]  # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
        "        ]  # ignore these, just a buffer\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
        "        ]  # same, just the mask (buffer)\n",
        "        transposed = [\n",
        "            \"attn.c_attn.weight\",\n",
        "            \"attn.c_proj.weight\",\n",
        "            \"mlp.c_fc.weight\",\n",
        "            \"mlp.c_proj.weight\",\n",
        "        ]\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(\n",
        "            sd_keys\n",
        "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(\n",
        "            f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
        "        )\n",
        "        print(\n",
        "            f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
        "        )\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            optim_groups, lr=learning_rate, betas=betas, **extra_args\n",
        "        )\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\"estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS\"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd // cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0 / dt)  # per second\n",
        "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = (\n",
        "                idx\n",
        "                if idx.size(1) <= self.config.block_size\n",
        "                else idx[:, -self.config.block_size :]\n",
        "            )\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "A5Vc2Ycc2-El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "hgo4Ffvf0tGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = drive_path + \"data\"\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True  # whether to decay the learning rate\n",
        "eval_only = False\n",
        "# DDP settings\n",
        "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
        "dtype = (\n",
        "    \"bfloat16\"\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else \"float16\"\n",
        ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [\n",
        "    k\n",
        "    for k, v in globals().items()\n",
        "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
        "]\n",
        "config = {k: globals()[k] for k in config_keys}  # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ[\"RANK\"])\n",
        "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "    device = f\"cuda:{ddp_local_rank}\"\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank  # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"float16\": torch.float16,\n",
        "}[dtype]\n",
        "ctx = (\n",
        "    nullcontext()\n",
        "    if device_type == \"cpu\"\n",
        "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        ")\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join(data_folder, dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if device_type == \"cuda\":\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = (\n",
        "            x.pin_memory().to(device, non_blocking=True),\n",
        "            y.pin_memory().to(device, non_blocking=True),\n",
        "        )\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, \"meta.pkl\")\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta[\"vocab_size\"]\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    pos_enc=pos_enc,\n",
        "    vocab_size=meta_vocab_size,\n",
        "    dropout=dropout,\n",
        ")  # start with model_args from command line\n",
        "if init_from == \"scratch\":\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\n",
        "            \"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\"\n",
        "        )\n",
        "    model_args[\"vocab_size\"] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == \"resume\":\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(drive_path,out_dir, \"ckpt.pt\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint[\"model_args\"]\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint[\"model\"]\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint[\"iter_num\"]\n",
        "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
        "elif init_from.startswith(\"gpt2\"):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args[\n",
        "        \"block_size\"\n",
        "    ] = block_size  # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay, learning_rate, (beta1, beta2), device_type\n",
        ")\n",
        "if init_from == \"resume\":\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "checkpoint = None  # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model)  # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch(\"train\")  # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0  # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model  # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(\n",
        "            f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        if wandb_log:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train/loss\": losses[\"train\"],\n",
        "                    \"val/loss\": losses[\"val\"],\n",
        "                    \"lr\": lr,\n",
        "                    \"mfu\": running_mfu * 100,  # convert to percentage\n",
        "                }\n",
        "            )\n",
        "        if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses[\"val\"]\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    \"model\": raw_model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"model_args\": model_args,\n",
        "                    \"iter_num\": iter_num,\n",
        "                    \"best_val_loss\": best_val_loss,\n",
        "                    \"config\": config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(drive_path + out_dir, \"ckpt.pt\"))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (\n",
        "                micro_step == gradient_accumulation_steps - 1\n",
        "            )\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = (\n",
        "                loss / gradient_accumulation_steps\n",
        "            )  # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch(\"train\")\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5:  # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
        "        print(\n",
        "            f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\"\n",
        "        )\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9YCDXC330xQr",
        "outputId": "d9d97a73-ac7a-4e01-af01-4cd85448ffdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 21 (inside /content/drive/MyDrive/bai/data/multiplication/meta.pkl)\n",
            "Resuming training from out-multi\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 38, with 10,739,328 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjeremy-herbst\u001b[0m (\u001b[33mbai_seminar_2324\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240116_230902-di1idgpy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bai_seminar_2324/multiplication-char/runs/di1idgpy' target=\"_blank\">relative_pos_enc_100k</a></strong> to <a href='https://wandb.ai/bai_seminar_2324/multiplication-char' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bai_seminar_2324/multiplication-char' target=\"_blank\">https://wandb.ai/bai_seminar_2324/multiplication-char</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bai_seminar_2324/multiplication-char/runs/di1idgpy' target=\"_blank\">https://wandb.ai/bai_seminar_2324/multiplication-char/runs/di1idgpy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 27250: train loss 0.5119, val loss 0.5137\n",
            "iter 27250: loss 0.5125, time 105712.30ms, mfu -100.00%\n",
            "iter 27300: loss 0.5124, time 389.07ms, mfu 0.96%\n",
            "iter 27350: loss 0.5097, time 387.77ms, mfu 0.96%\n",
            "iter 27400: loss 0.5169, time 395.74ms, mfu 0.96%\n",
            "iter 27450: loss 0.5218, time 393.90ms, mfu 0.96%\n",
            "step 27500: train loss 0.5130, val loss 0.5147\n",
            "iter 27500: loss 0.5137, time 20075.06ms, mfu 0.86%\n",
            "iter 27550: loss 0.5094, time 387.55ms, mfu 0.87%\n",
            "iter 27600: loss 0.5155, time 393.69ms, mfu 0.88%\n",
            "iter 27650: loss 0.5154, time 390.68ms, mfu 0.89%\n",
            "iter 27700: loss 0.5039, time 387.07ms, mfu 0.89%\n",
            "step 27750: train loss 0.5132, val loss 0.5144\n",
            "iter 27750: loss 0.5132, time 20289.84ms, mfu 0.81%\n",
            "iter 27800: loss 0.5135, time 392.05ms, mfu 0.82%\n",
            "iter 27850: loss 0.5096, time 390.76ms, mfu 0.83%\n",
            "iter 27900: loss 0.5072, time 387.96ms, mfu 0.85%\n",
            "iter 27950: loss 0.5119, time 392.22ms, mfu 0.86%\n",
            "step 28000: train loss 0.5111, val loss 0.5126\n",
            "saving checkpoint to out-multi\n",
            "iter 28000: loss 0.5140, time 20705.32ms, mfu 0.77%\n",
            "iter 28050: loss 0.5082, time 390.09ms, mfu 0.79%\n",
            "iter 28100: loss 0.5160, time 392.75ms, mfu 0.81%\n",
            "iter 28150: loss 0.5174, time 391.93ms, mfu 0.82%\n",
            "iter 28200: loss 0.5185, time 386.08ms, mfu 0.84%\n",
            "step 28250: train loss 0.5096, val loss 0.5113\n",
            "saving checkpoint to out-multi\n",
            "iter 28250: loss 0.5076, time 20781.52ms, mfu 0.75%\n",
            "iter 28300: loss 0.5112, time 394.83ms, mfu 0.77%\n",
            "iter 28350: loss 0.5077, time 391.26ms, mfu 0.79%\n",
            "iter 28400: loss 0.5134, time 390.96ms, mfu 0.81%\n",
            "iter 28450: loss 0.5101, time 388.98ms, mfu 0.82%\n",
            "step 28500: train loss 0.5101, val loss 0.5118\n",
            "iter 28500: loss 0.5107, time 20292.47ms, mfu 0.74%\n",
            "iter 28550: loss 0.5097, time 389.59ms, mfu 0.76%\n",
            "iter 28600: loss 0.5107, time 389.24ms, mfu 0.78%\n",
            "iter 28650: loss 0.5104, time 390.07ms, mfu 0.80%\n",
            "iter 28700: loss 0.5081, time 393.03ms, mfu 0.81%\n",
            "step 28750: train loss 0.5065, val loss 0.5077\n",
            "saving checkpoint to out-multi\n",
            "iter 28750: loss 0.5090, time 20780.10ms, mfu 0.74%\n",
            "iter 28800: loss 0.5053, time 389.27ms, mfu 0.76%\n",
            "iter 28850: loss 0.5105, time 387.59ms, mfu 0.78%\n",
            "iter 28900: loss 0.5083, time 391.28ms, mfu 0.80%\n",
            "iter 28950: loss 0.5096, time 392.38ms, mfu 0.81%\n",
            "step 29000: train loss 0.5066, val loss 0.5082\n",
            "iter 29000: loss 0.5031, time 20327.56ms, mfu 0.73%\n",
            "iter 29050: loss 0.5079, time 390.77ms, mfu 0.75%\n",
            "iter 29100: loss 0.5065, time 391.01ms, mfu 0.77%\n",
            "iter 29150: loss 0.5057, time 386.93ms, mfu 0.79%\n",
            "iter 29200: loss 0.5021, time 393.27ms, mfu 0.81%\n",
            "step 29250: train loss 0.5034, val loss 0.5051\n",
            "saving checkpoint to out-multi\n",
            "iter 29250: loss 0.5030, time 20698.90ms, mfu 0.73%\n",
            "iter 29300: loss 0.4983, time 390.16ms, mfu 0.75%\n",
            "iter 29350: loss 0.5009, time 386.92ms, mfu 0.77%\n",
            "iter 29400: loss 0.5020, time 390.86ms, mfu 0.79%\n",
            "iter 29450: loss 0.5030, time 387.32ms, mfu 0.81%\n",
            "step 29500: train loss 0.5013, val loss 0.5033\n",
            "saving checkpoint to out-multi\n",
            "iter 29500: loss 0.5022, time 21034.63ms, mfu 0.73%\n",
            "iter 29550: loss 0.4981, time 392.44ms, mfu 0.75%\n",
            "iter 29600: loss 0.4972, time 386.39ms, mfu 0.77%\n",
            "iter 29650: loss 0.5018, time 391.54ms, mfu 0.79%\n",
            "iter 29700: loss 0.5004, time 390.83ms, mfu 0.81%\n",
            "step 29750: train loss 0.5016, val loss 0.5031\n",
            "saving checkpoint to out-multi\n",
            "iter 29750: loss 0.4984, time 20938.19ms, mfu 0.73%\n",
            "iter 29800: loss 0.4998, time 393.68ms, mfu 0.75%\n",
            "iter 29850: loss 0.4937, time 388.25ms, mfu 0.77%\n",
            "iter 29900: loss 0.5016, time 395.45ms, mfu 0.79%\n",
            "iter 29950: loss 0.4985, time 394.01ms, mfu 0.80%\n",
            "step 30000: train loss 0.4990, val loss 0.4998\n",
            "saving checkpoint to out-multi\n",
            "iter 30000: loss 0.4976, time 20921.59ms, mfu 0.72%\n",
            "iter 30050: loss 0.4943, time 393.77ms, mfu 0.75%\n",
            "iter 30100: loss 0.4961, time 390.12ms, mfu 0.77%\n",
            "iter 30150: loss 0.4965, time 392.23ms, mfu 0.79%\n",
            "iter 30200: loss 0.4975, time 386.35ms, mfu 0.80%\n",
            "step 30250: train loss 0.4976, val loss 0.4986\n",
            "saving checkpoint to out-multi\n",
            "iter 30250: loss 0.4948, time 20950.58ms, mfu 0.73%\n",
            "iter 30300: loss 0.5002, time 393.13ms, mfu 0.75%\n",
            "iter 30350: loss 0.4996, time 394.01ms, mfu 0.77%\n",
            "iter 30400: loss 0.4997, time 387.04ms, mfu 0.79%\n",
            "iter 30450: loss 0.4972, time 392.11ms, mfu 0.80%\n",
            "step 30500: train loss 0.4972, val loss 0.4987\n",
            "iter 30500: loss 0.4937, time 20304.21ms, mfu 0.72%\n",
            "iter 30550: loss 0.4989, time 398.25ms, mfu 0.75%\n",
            "iter 30600: loss 0.4930, time 393.70ms, mfu 0.77%\n",
            "iter 30650: loss 0.5053, time 393.22ms, mfu 0.78%\n",
            "iter 30700: loss 0.4944, time 386.96ms, mfu 0.80%\n",
            "step 30750: train loss 0.4966, val loss 0.4977\n",
            "saving checkpoint to out-multi\n",
            "iter 30750: loss 0.4998, time 20621.38ms, mfu 0.72%\n",
            "iter 30800: loss 0.4960, time 388.78ms, mfu 0.75%\n",
            "iter 30850: loss 0.4972, time 391.55ms, mfu 0.77%\n",
            "iter 30900: loss 0.4925, time 392.66ms, mfu 0.79%\n",
            "iter 30950: loss 0.4979, time 393.15ms, mfu 0.80%\n",
            "step 31000: train loss 0.4949, val loss 0.4956\n",
            "saving checkpoint to out-multi\n",
            "iter 31000: loss 0.4960, time 20734.61ms, mfu 0.72%\n",
            "iter 31050: loss 0.4936, time 386.11ms, mfu 0.75%\n",
            "iter 31100: loss 0.4928, time 386.91ms, mfu 0.77%\n",
            "iter 31150: loss 0.4976, time 391.68ms, mfu 0.79%\n",
            "iter 31200: loss 0.5050, time 388.39ms, mfu 0.80%\n",
            "step 31250: train loss 0.4962, val loss 0.4978\n",
            "iter 31250: loss 0.4978, time 20333.27ms, mfu 0.73%\n",
            "iter 31300: loss 0.4908, time 387.75ms, mfu 0.75%\n",
            "iter 31350: loss 0.4989, time 392.76ms, mfu 0.77%\n",
            "iter 31400: loss 0.5005, time 387.85ms, mfu 0.79%\n",
            "iter 31450: loss 0.4902, time 386.23ms, mfu 0.81%\n",
            "step 31500: train loss 0.4965, val loss 0.4978\n",
            "iter 31500: loss 0.5019, time 20326.13ms, mfu 0.73%\n",
            "iter 31550: loss 0.4963, time 394.37ms, mfu 0.75%\n",
            "iter 31600: loss 0.4943, time 396.64ms, mfu 0.77%\n",
            "iter 31650: loss 0.4987, time 392.53ms, mfu 0.79%\n",
            "iter 31700: loss 0.4895, time 393.76ms, mfu 0.80%\n",
            "step 31750: train loss 0.4954, val loss 0.4971\n",
            "iter 31750: loss 0.4988, time 20307.01ms, mfu 0.72%\n",
            "iter 31800: loss 0.4945, time 385.76ms, mfu 0.75%\n",
            "iter 31850: loss 0.4936, time 386.63ms, mfu 0.77%\n",
            "iter 31900: loss 0.4979, time 395.31ms, mfu 0.79%\n",
            "iter 31950: loss 0.4956, time 390.90ms, mfu 0.80%\n",
            "step 32000: train loss 0.4953, val loss 0.4965\n",
            "iter 32000: loss 0.4955, time 20309.95ms, mfu 0.73%\n",
            "iter 32050: loss 0.4916, time 387.97ms, mfu 0.75%\n",
            "iter 32100: loss 0.4930, time 386.69ms, mfu 0.77%\n",
            "iter 32150: loss 0.4984, time 391.23ms, mfu 0.79%\n",
            "iter 32200: loss 0.4924, time 391.18ms, mfu 0.80%\n",
            "step 32250: train loss 0.4944, val loss 0.4963\n",
            "iter 32250: loss 0.4939, time 20339.56ms, mfu 0.73%\n",
            "iter 32300: loss 0.4931, time 392.47ms, mfu 0.75%\n",
            "iter 32350: loss 0.4997, time 386.88ms, mfu 0.77%\n",
            "iter 32400: loss 0.4895, time 389.29ms, mfu 0.79%\n",
            "iter 32450: loss 0.4984, time 393.80ms, mfu 0.80%\n",
            "step 32500: train loss 0.4952, val loss 0.4959\n",
            "iter 32500: loss 0.4976, time 20418.44ms, mfu 0.73%\n",
            "iter 32550: loss 0.4948, time 386.64ms, mfu 0.75%\n",
            "iter 32600: loss 0.4934, time 393.42ms, mfu 0.77%\n",
            "iter 32650: loss 0.4998, time 393.93ms, mfu 0.79%\n",
            "iter 32700: loss 0.4941, time 393.21ms, mfu 0.80%\n",
            "step 32750: train loss 0.4937, val loss 0.4955\n",
            "saving checkpoint to out-multi\n",
            "iter 32750: loss 0.4952, time 20733.58ms, mfu 0.72%\n",
            "iter 32800: loss 0.4937, time 391.45ms, mfu 0.75%\n",
            "iter 32850: loss 0.5003, time 389.20ms, mfu 0.77%\n",
            "iter 32900: loss 0.4992, time 390.89ms, mfu 0.79%\n",
            "iter 32950: loss 0.5059, time 389.69ms, mfu 0.80%\n",
            "step 33000: train loss 0.4936, val loss 0.4950\n",
            "saving checkpoint to out-multi\n",
            "iter 33000: loss 0.4961, time 20836.22ms, mfu 0.73%\n",
            "iter 33050: loss 0.4980, time 398.08ms, mfu 0.75%\n",
            "iter 33100: loss 0.4899, time 390.23ms, mfu 0.77%\n",
            "iter 33150: loss 0.4927, time 386.14ms, mfu 0.79%\n",
            "iter 33200: loss 0.4935, time 396.97ms, mfu 0.80%\n",
            "step 33250: train loss 0.4939, val loss 0.4948\n",
            "saving checkpoint to out-multi\n",
            "iter 33250: loss 0.4926, time 20814.51ms, mfu 0.72%\n",
            "iter 33300: loss 0.4883, time 390.47ms, mfu 0.75%\n",
            "iter 33350: loss 0.4961, time 386.70ms, mfu 0.77%\n",
            "iter 33400: loss 0.4940, time 386.89ms, mfu 0.79%\n",
            "iter 33450: loss 0.4958, time 388.93ms, mfu 0.80%\n",
            "step 33500: train loss 0.4938, val loss 0.4952\n",
            "iter 33500: loss 0.4981, time 20340.55ms, mfu 0.73%\n",
            "iter 33550: loss 0.4917, time 389.71ms, mfu 0.75%\n",
            "iter 33600: loss 0.5008, time 391.52ms, mfu 0.77%\n",
            "iter 33650: loss 0.4860, time 390.50ms, mfu 0.79%\n",
            "iter 33700: loss 0.4968, time 393.25ms, mfu 0.80%\n",
            "step 33750: train loss 0.4960, val loss 0.4976\n",
            "iter 33750: loss 0.4998, time 20373.33ms, mfu 0.73%\n",
            "iter 33800: loss 0.4925, time 396.38ms, mfu 0.75%\n",
            "iter 33850: loss 0.4918, time 392.77ms, mfu 0.77%\n",
            "iter 33900: loss 0.4971, time 390.51ms, mfu 0.79%\n",
            "iter 33950: loss 0.4898, time 393.34ms, mfu 0.80%\n",
            "step 34000: train loss 0.4935, val loss 0.4954\n",
            "iter 34000: loss 0.4946, time 20400.61ms, mfu 0.72%\n",
            "iter 34050: loss 0.4949, time 387.06ms, mfu 0.75%\n",
            "iter 34100: loss 0.4955, time 393.53ms, mfu 0.77%\n",
            "iter 34150: loss 0.4861, time 393.54ms, mfu 0.79%\n",
            "iter 34200: loss 0.5049, time 390.81ms, mfu 0.80%\n",
            "step 34250: train loss 0.4948, val loss 0.4960\n",
            "iter 34250: loss 0.4932, time 20371.68ms, mfu 0.72%\n",
            "iter 34300: loss 0.4928, time 393.94ms, mfu 0.75%\n",
            "iter 34350: loss 0.4936, time 392.85ms, mfu 0.77%\n",
            "iter 34400: loss 0.4903, time 390.76ms, mfu 0.78%\n",
            "iter 34450: loss 0.4970, time 393.84ms, mfu 0.80%\n",
            "step 34500: train loss 0.4946, val loss 0.4960\n",
            "iter 34500: loss 0.4971, time 20339.24ms, mfu 0.72%\n",
            "iter 34550: loss 0.4955, time 392.94ms, mfu 0.75%\n",
            "iter 34600: loss 0.4927, time 389.66ms, mfu 0.77%\n",
            "iter 34650: loss 0.4952, time 389.02ms, mfu 0.79%\n",
            "iter 34700: loss 0.5008, time 393.28ms, mfu 0.80%\n",
            "step 34750: train loss 0.4953, val loss 0.4971\n",
            "iter 34750: loss 0.4946, time 20353.55ms, mfu 0.72%\n",
            "iter 34800: loss 0.4891, time 387.20ms, mfu 0.75%\n",
            "iter 34850: loss 0.4845, time 390.50ms, mfu 0.77%\n",
            "iter 34900: loss 0.4894, time 393.71ms, mfu 0.79%\n",
            "iter 34950: loss 0.5039, time 392.65ms, mfu 0.80%\n",
            "step 35000: train loss 0.4950, val loss 0.4968\n",
            "iter 35000: loss 0.4939, time 20377.33ms, mfu 0.72%\n",
            "iter 35050: loss 0.4912, time 390.74ms, mfu 0.75%\n",
            "iter 35100: loss 0.4997, time 393.46ms, mfu 0.77%\n",
            "iter 35150: loss 0.4934, time 386.36ms, mfu 0.79%\n",
            "iter 35200: loss 0.4885, time 394.12ms, mfu 0.80%\n",
            "step 35250: train loss 0.4922, val loss 0.4943\n",
            "saving checkpoint to out-multi\n",
            "iter 35250: loss 0.4913, time 20817.34ms, mfu 0.72%\n",
            "iter 35300: loss 0.4944, time 395.08ms, mfu 0.75%\n",
            "iter 35350: loss 0.4966, time 390.19ms, mfu 0.77%\n",
            "iter 35400: loss 0.4977, time 392.41ms, mfu 0.79%\n",
            "iter 35450: loss 0.4908, time 392.09ms, mfu 0.80%\n",
            "step 35500: train loss 0.4969, val loss 0.4984\n",
            "iter 35500: loss 0.4905, time 20427.31ms, mfu 0.72%\n",
            "iter 35550: loss 0.4949, time 394.01ms, mfu 0.75%\n",
            "iter 35600: loss 0.4933, time 392.82ms, mfu 0.77%\n",
            "iter 35650: loss 0.4922, time 391.05ms, mfu 0.78%\n",
            "iter 35700: loss 0.4968, time 391.62ms, mfu 0.80%\n",
            "step 35750: train loss 0.4947, val loss 0.4956\n",
            "iter 35750: loss 0.4958, time 20386.62ms, mfu 0.72%\n",
            "iter 35800: loss 0.4921, time 389.86ms, mfu 0.75%\n",
            "iter 35850: loss 0.4910, time 394.26ms, mfu 0.77%\n",
            "iter 35900: loss 0.4925, time 386.80ms, mfu 0.79%\n",
            "iter 35950: loss 0.5038, time 388.70ms, mfu 0.80%\n",
            "step 36000: train loss 0.4919, val loss 0.4935\n",
            "saving checkpoint to out-multi\n",
            "iter 36000: loss 0.4968, time 20894.39ms, mfu 0.72%\n",
            "iter 36050: loss 0.4867, time 391.74ms, mfu 0.75%\n",
            "iter 36100: loss 0.4923, time 392.80ms, mfu 0.77%\n",
            "iter 36150: loss 0.4944, time 393.66ms, mfu 0.79%\n",
            "iter 36200: loss 0.4864, time 394.65ms, mfu 0.80%\n",
            "step 36250: train loss 0.4926, val loss 0.4943\n",
            "iter 36250: loss 0.4953, time 20393.68ms, mfu 0.72%\n",
            "iter 36300: loss 0.4905, time 389.38ms, mfu 0.75%\n",
            "iter 36350: loss 0.4866, time 385.52ms, mfu 0.77%\n",
            "iter 36400: loss 0.4926, time 392.98ms, mfu 0.79%\n",
            "iter 36450: loss 0.4920, time 388.90ms, mfu 0.80%\n",
            "step 36500: train loss 0.4935, val loss 0.4953\n",
            "iter 36500: loss 0.4888, time 20374.20ms, mfu 0.72%\n",
            "iter 36550: loss 0.4893, time 389.99ms, mfu 0.75%\n",
            "iter 36600: loss 0.4911, time 389.24ms, mfu 0.77%\n",
            "iter 36650: loss 0.4997, time 399.02ms, mfu 0.79%\n",
            "iter 36700: loss 0.4916, time 392.72ms, mfu 0.80%\n",
            "step 36750: train loss 0.4942, val loss 0.4955\n",
            "iter 36750: loss 0.4895, time 20278.15ms, mfu 0.72%\n",
            "iter 36800: loss 0.4959, time 387.46ms, mfu 0.75%\n",
            "iter 36850: loss 0.4877, time 390.25ms, mfu 0.77%\n",
            "iter 36900: loss 0.4892, time 393.02ms, mfu 0.79%\n",
            "iter 36950: loss 0.4904, time 391.44ms, mfu 0.80%\n",
            "step 37000: train loss 0.4920, val loss 0.4934\n",
            "saving checkpoint to out-multi\n",
            "iter 37000: loss 0.4907, time 20727.94ms, mfu 0.72%\n",
            "iter 37050: loss 0.4953, time 392.89ms, mfu 0.75%\n",
            "iter 37100: loss 0.4880, time 385.74ms, mfu 0.77%\n",
            "iter 37150: loss 0.4916, time 393.54ms, mfu 0.79%\n",
            "iter 37200: loss 0.5006, time 392.33ms, mfu 0.80%\n",
            "step 37250: train loss 0.4928, val loss 0.4948\n",
            "iter 37250: loss 0.4933, time 20419.32ms, mfu 0.72%\n",
            "iter 37300: loss 0.4950, time 395.10ms, mfu 0.75%\n",
            "iter 37350: loss 0.4907, time 387.42ms, mfu 0.77%\n",
            "iter 37400: loss 0.4973, time 390.81ms, mfu 0.79%\n",
            "iter 37450: loss 0.4852, time 395.03ms, mfu 0.80%\n",
            "step 37500: train loss 0.4925, val loss 0.4939\n",
            "iter 37500: loss 0.4940, time 20352.17ms, mfu 0.72%\n",
            "iter 37550: loss 0.4949, time 395.24ms, mfu 0.75%\n",
            "iter 37600: loss 0.4983, time 398.73ms, mfu 0.76%\n",
            "iter 37650: loss 0.4934, time 387.02ms, mfu 0.78%\n",
            "iter 37700: loss 0.4941, time 390.93ms, mfu 0.80%\n",
            "step 37750: train loss 0.4927, val loss 0.4941\n",
            "iter 37750: loss 0.4948, time 20284.18ms, mfu 0.72%\n",
            "iter 37800: loss 0.4855, time 392.31ms, mfu 0.75%\n",
            "iter 37850: loss 0.4976, time 391.49ms, mfu 0.77%\n",
            "iter 37900: loss 0.4910, time 388.68ms, mfu 0.79%\n",
            "iter 37950: loss 0.4948, time 387.53ms, mfu 0.80%\n",
            "step 38000: train loss 0.4989, val loss 0.5008\n",
            "iter 38000: loss 0.4974, time 20408.84ms, mfu 0.72%\n",
            "iter 38050: loss 0.5008, time 386.72ms, mfu 0.75%\n",
            "iter 38100: loss 0.4984, time 387.51ms, mfu 0.77%\n",
            "iter 38150: loss 0.5082, time 389.00ms, mfu 0.79%\n",
            "iter 38200: loss 0.4953, time 390.52ms, mfu 0.81%\n",
            "step 38250: train loss 0.4905, val loss 0.4921\n",
            "saving checkpoint to out-multi\n",
            "iter 38250: loss 0.4942, time 20774.86ms, mfu 0.73%\n",
            "iter 38300: loss 0.4899, time 388.64ms, mfu 0.75%\n",
            "iter 38350: loss 0.4878, time 393.00ms, mfu 0.77%\n",
            "iter 38400: loss 0.4895, time 393.43ms, mfu 0.79%\n",
            "iter 38450: loss 0.4892, time 387.10ms, mfu 0.80%\n",
            "step 38500: train loss 0.4941, val loss 0.4955\n",
            "iter 38500: loss 0.4931, time 20400.08ms, mfu 0.73%\n",
            "iter 38550: loss 0.5004, time 393.66ms, mfu 0.75%\n",
            "iter 38600: loss 0.4976, time 386.31ms, mfu 0.77%\n",
            "iter 38650: loss 0.4946, time 394.42ms, mfu 0.79%\n",
            "iter 38700: loss 0.5008, time 388.27ms, mfu 0.80%\n",
            "step 38750: train loss 0.4926, val loss 0.4940\n",
            "iter 38750: loss 0.4914, time 20322.54ms, mfu 0.73%\n",
            "iter 38800: loss 0.4958, time 392.41ms, mfu 0.75%\n",
            "iter 38850: loss 0.4910, time 389.06ms, mfu 0.77%\n",
            "iter 38900: loss 0.4932, time 392.62ms, mfu 0.79%\n",
            "iter 38950: loss 0.4893, time 392.84ms, mfu 0.80%\n",
            "step 39000: train loss 0.4897, val loss 0.4915\n",
            "saving checkpoint to out-multi\n",
            "iter 39000: loss 0.4878, time 20721.68ms, mfu 0.72%\n",
            "iter 39050: loss 0.4888, time 391.28ms, mfu 0.75%\n",
            "iter 39100: loss 0.4944, time 393.57ms, mfu 0.77%\n",
            "iter 39150: loss 0.4901, time 390.46ms, mfu 0.79%\n",
            "iter 39200: loss 0.4947, time 393.60ms, mfu 0.80%\n",
            "step 39250: train loss 0.4934, val loss 0.4943\n",
            "iter 39250: loss 0.4938, time 20333.18ms, mfu 0.72%\n",
            "iter 39300: loss 0.4967, time 390.86ms, mfu 0.75%\n",
            "iter 39350: loss 0.4934, time 390.80ms, mfu 0.77%\n",
            "iter 39400: loss 0.4944, time 392.57ms, mfu 0.79%\n",
            "iter 39450: loss 0.4885, time 394.77ms, mfu 0.80%\n",
            "step 39500: train loss 0.4918, val loss 0.4942\n",
            "iter 39500: loss 0.4910, time 20445.56ms, mfu 0.72%\n",
            "iter 39550: loss 0.4925, time 389.05ms, mfu 0.75%\n",
            "iter 39600: loss 0.4906, time 394.08ms, mfu 0.77%\n",
            "iter 39650: loss 0.4887, time 388.62ms, mfu 0.79%\n",
            "iter 39700: loss 0.4979, time 389.00ms, mfu 0.80%\n",
            "step 39750: train loss 0.4896, val loss 0.4912\n",
            "saving checkpoint to out-multi\n",
            "iter 39750: loss 0.4912, time 20721.86ms, mfu 0.72%\n",
            "iter 39800: loss 0.4914, time 392.99ms, mfu 0.75%\n",
            "iter 39850: loss 0.4849, time 396.33ms, mfu 0.77%\n",
            "iter 39900: loss 0.4952, time 394.09ms, mfu 0.78%\n",
            "iter 39950: loss 0.4943, time 389.79ms, mfu 0.80%\n",
            "step 40000: train loss 0.4901, val loss 0.4913\n",
            "iter 40000: loss 0.4844, time 20294.00ms, mfu 0.72%\n",
            "iter 40050: loss 0.4870, time 388.72ms, mfu 0.75%\n",
            "iter 40100: loss 0.4936, time 393.27ms, mfu 0.77%\n",
            "iter 40150: loss 0.4896, time 391.63ms, mfu 0.79%\n",
            "iter 40200: loss 0.4926, time 392.29ms, mfu 0.80%\n",
            "step 40250: train loss 0.4928, val loss 0.4943\n",
            "iter 40250: loss 0.4922, time 20406.24ms, mfu 0.72%\n",
            "iter 40300: loss 0.4934, time 392.82ms, mfu 0.75%\n",
            "iter 40350: loss 0.4902, time 393.52ms, mfu 0.77%\n",
            "iter 40400: loss 0.4851, time 391.39ms, mfu 0.78%\n",
            "iter 40450: loss 0.4948, time 392.62ms, mfu 0.80%\n",
            "step 40500: train loss 0.4896, val loss 0.4915\n",
            "iter 40500: loss 0.4881, time 20337.52ms, mfu 0.72%\n",
            "iter 40550: loss 0.4912, time 391.63ms, mfu 0.75%\n",
            "iter 40600: loss 0.4937, time 391.86ms, mfu 0.77%\n",
            "iter 40650: loss 0.4905, time 393.99ms, mfu 0.78%\n",
            "iter 40700: loss 0.4950, time 386.23ms, mfu 0.80%\n",
            "step 40750: train loss 0.4921, val loss 0.4943\n",
            "iter 40750: loss 0.4915, time 20317.84ms, mfu 0.72%\n",
            "iter 40800: loss 0.4928, time 395.18ms, mfu 0.75%\n",
            "iter 40850: loss 0.4865, time 390.95ms, mfu 0.77%\n",
            "iter 40900: loss 0.4880, time 392.96ms, mfu 0.78%\n",
            "iter 40950: loss 0.4897, time 386.44ms, mfu 0.80%\n",
            "step 41000: train loss 0.4950, val loss 0.4963\n",
            "iter 41000: loss 0.4976, time 20320.46ms, mfu 0.72%\n",
            "iter 41050: loss 0.4867, time 391.18ms, mfu 0.75%\n",
            "iter 41100: loss 0.4849, time 393.40ms, mfu 0.77%\n",
            "iter 41150: loss 0.4903, time 390.31ms, mfu 0.79%\n",
            "iter 41200: loss 0.4881, time 389.89ms, mfu 0.80%\n",
            "step 41250: train loss 0.4892, val loss 0.4902\n",
            "saving checkpoint to out-multi\n",
            "iter 41250: loss 0.4870, time 20878.48ms, mfu 0.72%\n",
            "iter 41300: loss 0.4904, time 389.43ms, mfu 0.75%\n",
            "iter 41350: loss 0.4929, time 394.01ms, mfu 0.77%\n",
            "iter 41400: loss 0.4860, time 389.81ms, mfu 0.79%\n",
            "iter 41450: loss 0.4900, time 393.03ms, mfu 0.80%\n",
            "step 41500: train loss 0.4904, val loss 0.4920\n",
            "iter 41500: loss 0.4887, time 20330.07ms, mfu 0.72%\n",
            "iter 41550: loss 0.4924, time 390.49ms, mfu 0.75%\n",
            "iter 41600: loss 0.4871, time 386.48ms, mfu 0.77%\n",
            "iter 41650: loss 0.4878, time 388.39ms, mfu 0.79%\n",
            "iter 41700: loss 0.4827, time 388.40ms, mfu 0.80%\n",
            "step 41750: train loss 0.4902, val loss 0.4917\n",
            "iter 41750: loss 0.4902, time 20309.08ms, mfu 0.73%\n",
            "iter 41800: loss 0.4947, time 391.13ms, mfu 0.75%\n",
            "iter 41850: loss 0.4883, time 392.20ms, mfu 0.77%\n",
            "iter 41900: loss 0.4926, time 394.07ms, mfu 0.79%\n",
            "iter 41950: loss 0.4938, time 389.86ms, mfu 0.80%\n",
            "step 42000: train loss 0.4941, val loss 0.4956\n",
            "iter 42000: loss 0.5007, time 20287.49ms, mfu 0.73%\n",
            "iter 42050: loss 0.4904, time 393.29ms, mfu 0.75%\n",
            "iter 42100: loss 0.4940, time 389.59ms, mfu 0.77%\n",
            "iter 42150: loss 0.4875, time 390.52ms, mfu 0.79%\n",
            "iter 42200: loss 0.4955, time 390.59ms, mfu 0.80%\n",
            "step 42250: train loss 0.4902, val loss 0.4920\n",
            "iter 42250: loss 0.4920, time 20372.49ms, mfu 0.72%\n",
            "iter 42300: loss 0.4933, time 393.18ms, mfu 0.75%\n",
            "iter 42350: loss 0.4916, time 388.20ms, mfu 0.77%\n",
            "iter 42400: loss 0.4956, time 386.69ms, mfu 0.79%\n",
            "iter 42450: loss 0.4934, time 393.80ms, mfu 0.80%\n",
            "step 42500: train loss 0.4914, val loss 0.4937\n",
            "iter 42500: loss 0.4877, time 20291.06ms, mfu 0.73%\n",
            "iter 42550: loss 0.4968, time 390.53ms, mfu 0.75%\n",
            "iter 42600: loss 0.4939, time 390.31ms, mfu 0.77%\n",
            "iter 42650: loss 0.4958, time 385.97ms, mfu 0.79%\n",
            "iter 42700: loss 0.4922, time 388.72ms, mfu 0.81%\n",
            "step 42750: train loss 0.4947, val loss 0.4960\n",
            "iter 42750: loss 0.4955, time 20377.22ms, mfu 0.73%\n",
            "iter 42800: loss 0.4968, time 393.67ms, mfu 0.75%\n",
            "iter 42850: loss 0.4893, time 394.98ms, mfu 0.77%\n",
            "iter 42900: loss 0.4912, time 389.28ms, mfu 0.79%\n",
            "iter 42950: loss 0.4891, time 392.56ms, mfu 0.80%\n",
            "step 43000: train loss 0.4928, val loss 0.4945\n",
            "iter 43000: loss 0.4942, time 20364.41ms, mfu 0.72%\n",
            "iter 43050: loss 0.4842, time 391.42ms, mfu 0.75%\n",
            "iter 43100: loss 0.4888, time 393.01ms, mfu 0.77%\n",
            "iter 43150: loss 0.4905, time 388.23ms, mfu 0.79%\n",
            "iter 43200: loss 0.4883, time 390.32ms, mfu 0.80%\n",
            "step 43250: train loss 0.4899, val loss 0.4914\n",
            "iter 43250: loss 0.4838, time 20325.09ms, mfu 0.73%\n",
            "iter 43300: loss 0.4879, time 390.26ms, mfu 0.75%\n",
            "iter 43350: loss 0.4970, time 388.24ms, mfu 0.77%\n",
            "iter 43400: loss 0.4909, time 387.80ms, mfu 0.79%\n",
            "iter 43450: loss 0.4910, time 388.80ms, mfu 0.81%\n",
            "step 43500: train loss 0.4881, val loss 0.4894\n",
            "saving checkpoint to out-multi\n",
            "iter 43500: loss 0.4915, time 20717.44ms, mfu 0.73%\n",
            "iter 43550: loss 0.4895, time 391.02ms, mfu 0.75%\n",
            "iter 43600: loss 0.4869, time 387.76ms, mfu 0.77%\n",
            "iter 43650: loss 0.4904, time 398.09ms, mfu 0.79%\n",
            "iter 43700: loss 0.4951, time 395.39ms, mfu 0.80%\n",
            "step 43750: train loss 0.4970, val loss 0.4992\n",
            "iter 43750: loss 0.4972, time 20383.97ms, mfu 0.72%\n",
            "iter 43800: loss 0.4937, time 394.27ms, mfu 0.75%\n",
            "iter 43850: loss 0.4921, time 389.06ms, mfu 0.77%\n",
            "iter 43900: loss 0.4903, time 392.97ms, mfu 0.79%\n",
            "iter 43950: loss 0.4918, time 386.54ms, mfu 0.80%\n",
            "step 44000: train loss 0.4882, val loss 0.4892\n",
            "saving checkpoint to out-multi\n",
            "iter 44000: loss 0.4875, time 20791.93ms, mfu 0.72%\n",
            "iter 44050: loss 0.4858, time 394.09ms, mfu 0.75%\n",
            "iter 44100: loss 0.4846, time 391.88ms, mfu 0.77%\n",
            "iter 44150: loss 0.4823, time 392.75ms, mfu 0.79%\n",
            "iter 44200: loss 0.4892, time 387.36ms, mfu 0.80%\n",
            "step 44250: train loss 0.4876, val loss 0.4892\n",
            "iter 44250: loss 0.4888, time 20377.32ms, mfu 0.72%\n",
            "iter 44300: loss 0.4890, time 388.29ms, mfu 0.75%\n",
            "iter 44350: loss 0.4873, time 394.81ms, mfu 0.77%\n",
            "iter 44400: loss 0.4831, time 392.74ms, mfu 0.79%\n",
            "iter 44450: loss 0.4895, time 386.35ms, mfu 0.80%\n",
            "step 44500: train loss 0.4878, val loss 0.4891\n",
            "saving checkpoint to out-multi\n",
            "iter 44500: loss 0.4867, time 20916.96ms, mfu 0.73%\n",
            "iter 44550: loss 0.4869, time 394.99ms, mfu 0.75%\n",
            "iter 44600: loss 0.4853, time 387.17ms, mfu 0.77%\n",
            "iter 44650: loss 0.4894, time 391.36ms, mfu 0.79%\n",
            "iter 44700: loss 0.4896, time 393.18ms, mfu 0.80%\n",
            "step 44750: train loss 0.4870, val loss 0.4889\n",
            "saving checkpoint to out-multi\n",
            "iter 44750: loss 0.4869, time 20822.60ms, mfu 0.72%\n",
            "iter 44800: loss 0.4910, time 391.66ms, mfu 0.75%\n",
            "iter 44850: loss 0.4845, time 391.48ms, mfu 0.77%\n",
            "iter 44900: loss 0.4862, time 387.92ms, mfu 0.79%\n",
            "iter 44950: loss 0.4843, time 393.09ms, mfu 0.80%\n",
            "step 45000: train loss 0.4863, val loss 0.4877\n",
            "saving checkpoint to out-multi\n",
            "iter 45000: loss 0.4840, time 20917.69ms, mfu 0.72%\n",
            "iter 45050: loss 0.4887, time 393.32ms, mfu 0.75%\n",
            "iter 45100: loss 0.4905, time 390.72ms, mfu 0.77%\n",
            "iter 45150: loss 0.4866, time 392.45ms, mfu 0.79%\n",
            "iter 45200: loss 0.4869, time 385.21ms, mfu 0.80%\n",
            "step 45250: train loss 0.4866, val loss 0.4879\n",
            "iter 45250: loss 0.4860, time 20355.19ms, mfu 0.73%\n",
            "iter 45300: loss 0.4857, time 394.13ms, mfu 0.75%\n",
            "iter 45350: loss 0.4920, time 393.44ms, mfu 0.77%\n",
            "iter 45400: loss 0.4824, time 397.60ms, mfu 0.78%\n",
            "iter 45450: loss 0.4857, time 392.72ms, mfu 0.80%\n",
            "step 45500: train loss 0.4868, val loss 0.4883\n",
            "iter 45500: loss 0.4829, time 20351.92ms, mfu 0.72%\n",
            "iter 45550: loss 0.4816, time 392.78ms, mfu 0.75%\n",
            "iter 45600: loss 0.4875, time 392.55ms, mfu 0.77%\n",
            "iter 45650: loss 0.4908, time 390.62ms, mfu 0.78%\n",
            "iter 45700: loss 0.4828, time 390.38ms, mfu 0.80%\n",
            "step 45750: train loss 0.4872, val loss 0.4889\n",
            "iter 45750: loss 0.4874, time 20381.40ms, mfu 0.72%\n",
            "iter 45800: loss 0.4889, time 392.28ms, mfu 0.75%\n",
            "iter 45850: loss 0.4883, time 388.62ms, mfu 0.77%\n",
            "iter 45900: loss 0.4874, time 387.63ms, mfu 0.79%\n",
            "iter 45950: loss 0.4852, time 412.27ms, mfu 0.80%\n",
            "step 46000: train loss 0.4866, val loss 0.4879\n",
            "iter 46000: loss 0.4889, time 20281.07ms, mfu 0.72%\n",
            "iter 46050: loss 0.4870, time 388.03ms, mfu 0.74%\n",
            "iter 46100: loss 0.4898, time 393.96ms, mfu 0.76%\n",
            "iter 46150: loss 0.4880, time 387.06ms, mfu 0.78%\n",
            "iter 46200: loss 0.4873, time 386.37ms, mfu 0.80%\n",
            "step 46250: train loss 0.4860, val loss 0.4878\n",
            "iter 46250: loss 0.4849, time 20297.38ms, mfu 0.72%\n",
            "iter 46300: loss 0.4895, time 392.66ms, mfu 0.75%\n",
            "iter 46350: loss 0.4825, time 391.72ms, mfu 0.77%\n",
            "iter 46400: loss 0.4835, time 391.16ms, mfu 0.79%\n",
            "iter 46450: loss 0.4872, time 387.89ms, mfu 0.80%\n",
            "step 46500: train loss 0.4864, val loss 0.4875\n",
            "saving checkpoint to out-multi\n",
            "iter 46500: loss 0.4882, time 20724.25ms, mfu 0.72%\n",
            "iter 46550: loss 0.4878, time 391.10ms, mfu 0.75%\n",
            "iter 46600: loss 0.4829, time 387.08ms, mfu 0.77%\n",
            "iter 46650: loss 0.4871, time 386.68ms, mfu 0.79%\n",
            "iter 46700: loss 0.4899, time 393.37ms, mfu 0.80%\n",
            "step 46750: train loss 0.4872, val loss 0.4892\n",
            "iter 46750: loss 0.4864, time 20376.66ms, mfu 0.73%\n",
            "iter 46800: loss 0.4911, time 386.22ms, mfu 0.75%\n",
            "iter 46850: loss 0.4946, time 388.47ms, mfu 0.77%\n",
            "iter 46900: loss 0.4800, time 392.62ms, mfu 0.79%\n",
            "iter 46950: loss 0.4878, time 386.89ms, mfu 0.81%\n",
            "step 47000: train loss 0.4869, val loss 0.4882\n",
            "iter 47000: loss 0.4849, time 20312.44ms, mfu 0.73%\n",
            "iter 47050: loss 0.4864, time 395.73ms, mfu 0.75%\n",
            "iter 47100: loss 0.4830, time 393.94ms, mfu 0.77%\n",
            "iter 47150: loss 0.4816, time 393.61ms, mfu 0.79%\n",
            "iter 47200: loss 0.4864, time 393.83ms, mfu 0.80%\n",
            "step 47250: train loss 0.4852, val loss 0.4862\n",
            "saving checkpoint to out-multi\n",
            "iter 47250: loss 0.4853, time 20795.06ms, mfu 0.72%\n",
            "iter 47300: loss 0.4859, time 393.92ms, mfu 0.75%\n",
            "iter 47350: loss 0.4860, time 390.90ms, mfu 0.77%\n",
            "iter 47400: loss 0.4836, time 392.22ms, mfu 0.78%\n",
            "iter 47450: loss 0.4883, time 390.80ms, mfu 0.80%\n",
            "step 47500: train loss 0.4863, val loss 0.4876\n",
            "iter 47500: loss 0.4866, time 20388.92ms, mfu 0.72%\n",
            "iter 47550: loss 0.4824, time 388.55ms, mfu 0.75%\n",
            "iter 47600: loss 0.4846, time 390.99ms, mfu 0.77%\n",
            "iter 47650: loss 0.4903, time 394.73ms, mfu 0.79%\n",
            "iter 47700: loss 0.4935, time 389.43ms, mfu 0.80%\n",
            "step 47750: train loss 0.4856, val loss 0.4871\n",
            "iter 47750: loss 0.4867, time 20282.52ms, mfu 0.72%\n",
            "iter 47800: loss 0.4895, time 390.68ms, mfu 0.75%\n",
            "iter 47850: loss 0.4844, time 386.39ms, mfu 0.77%\n",
            "iter 47900: loss 0.4913, time 386.54ms, mfu 0.79%\n",
            "iter 47950: loss 0.4848, time 390.24ms, mfu 0.80%\n",
            "step 48000: train loss 0.4883, val loss 0.4896\n",
            "iter 48000: loss 0.4908, time 20298.17ms, mfu 0.73%\n",
            "iter 48050: loss 0.4888, time 389.37ms, mfu 0.75%\n",
            "iter 48100: loss 0.4868, time 392.32ms, mfu 0.77%\n",
            "iter 48150: loss 0.4836, time 394.52ms, mfu 0.79%\n",
            "iter 48200: loss 0.4829, time 389.76ms, mfu 0.80%\n",
            "step 48250: train loss 0.4860, val loss 0.4883\n",
            "iter 48250: loss 0.4812, time 20380.32ms, mfu 0.73%\n",
            "iter 48300: loss 0.4849, time 391.09ms, mfu 0.75%\n",
            "iter 48350: loss 0.4852, time 394.49ms, mfu 0.77%\n",
            "iter 48400: loss 0.4887, time 388.94ms, mfu 0.79%\n",
            "iter 48450: loss 0.4827, time 392.32ms, mfu 0.80%\n",
            "step 48500: train loss 0.4852, val loss 0.4864\n",
            "iter 48500: loss 0.4834, time 20360.80ms, mfu 0.72%\n",
            "iter 48550: loss 0.4880, time 397.75ms, mfu 0.75%\n",
            "iter 48600: loss 0.4806, time 392.50ms, mfu 0.77%\n",
            "iter 48650: loss 0.4832, time 393.75ms, mfu 0.78%\n",
            "iter 48700: loss 0.4837, time 390.42ms, mfu 0.80%\n",
            "step 48750: train loss 0.4851, val loss 0.4869\n",
            "iter 48750: loss 0.4860, time 20381.16ms, mfu 0.72%\n",
            "iter 48800: loss 0.4811, time 394.64ms, mfu 0.74%\n",
            "iter 48850: loss 0.4897, time 393.47ms, mfu 0.77%\n",
            "iter 48900: loss 0.4892, time 386.66ms, mfu 0.79%\n",
            "iter 48950: loss 0.4821, time 386.75ms, mfu 0.80%\n",
            "step 49000: train loss 0.4843, val loss 0.4857\n",
            "saving checkpoint to out-multi\n",
            "iter 49000: loss 0.4875, time 20852.06ms, mfu 0.72%\n",
            "iter 49050: loss 0.4787, time 393.93ms, mfu 0.75%\n",
            "iter 49100: loss 0.4873, time 393.94ms, mfu 0.77%\n",
            "iter 49150: loss 0.4815, time 392.67ms, mfu 0.78%\n",
            "iter 49200: loss 0.4834, time 388.83ms, mfu 0.80%\n",
            "step 49250: train loss 0.4846, val loss 0.4867\n",
            "iter 49250: loss 0.4903, time 20382.13ms, mfu 0.72%\n",
            "iter 49300: loss 0.4934, time 391.63ms, mfu 0.75%\n",
            "iter 49350: loss 0.4877, time 388.82ms, mfu 0.77%\n",
            "iter 49400: loss 0.4850, time 393.34ms, mfu 0.79%\n",
            "iter 49450: loss 0.4890, time 391.11ms, mfu 0.80%\n",
            "step 49500: train loss 0.4847, val loss 0.4857\n",
            "saving checkpoint to out-multi\n",
            "iter 49500: loss 0.4903, time 20885.57ms, mfu 0.72%\n",
            "iter 49550: loss 0.4879, time 391.41ms, mfu 0.75%\n",
            "iter 49600: loss 0.4851, time 391.59ms, mfu 0.77%\n",
            "iter 49650: loss 0.4839, time 392.78ms, mfu 0.79%\n",
            "iter 49700: loss 0.4802, time 390.02ms, mfu 0.80%\n",
            "step 49750: train loss 0.4843, val loss 0.4859\n",
            "iter 49750: loss 0.4869, time 20417.18ms, mfu 0.72%\n",
            "iter 49800: loss 0.4847, time 392.53ms, mfu 0.75%\n",
            "iter 49850: loss 0.4872, time 391.53ms, mfu 0.77%\n",
            "iter 49900: loss 0.4835, time 391.74ms, mfu 0.79%\n",
            "iter 49950: loss 0.4791, time 389.87ms, mfu 0.80%\n",
            "step 50000: train loss 0.4846, val loss 0.4863\n",
            "iter 50000: loss 0.4800, time 20370.79ms, mfu 0.72%\n",
            "iter 50050: loss 0.4873, time 389.18ms, mfu 0.75%\n",
            "iter 50100: loss 0.4829, time 387.40ms, mfu 0.77%\n",
            "iter 50150: loss 0.4812, time 393.86ms, mfu 0.79%\n",
            "iter 50200: loss 0.4906, time 387.85ms, mfu 0.80%\n",
            "step 50250: train loss 0.4846, val loss 0.4862\n",
            "iter 50250: loss 0.4821, time 20323.40ms, mfu 0.73%\n",
            "iter 50300: loss 0.4896, time 390.36ms, mfu 0.75%\n",
            "iter 50350: loss 0.4867, time 391.26ms, mfu 0.77%\n",
            "iter 50400: loss 0.4871, time 394.92ms, mfu 0.79%\n",
            "iter 50450: loss 0.4823, time 389.22ms, mfu 0.80%\n",
            "step 50500: train loss 0.4838, val loss 0.4858\n",
            "iter 50500: loss 0.4850, time 20386.47ms, mfu 0.72%\n",
            "iter 50550: loss 0.4841, time 387.92ms, mfu 0.75%\n",
            "iter 50600: loss 0.4783, time 394.14ms, mfu 0.77%\n",
            "iter 50650: loss 0.4872, time 395.69ms, mfu 0.79%\n",
            "iter 50700: loss 0.4866, time 390.26ms, mfu 0.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = drive_path + \"data\"\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True  # whether to decay the learning rate\n",
        "eval_only = False\n",
        "# DDP settings\n",
        "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
        "dtype = (\n",
        "    \"bfloat16\"\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else \"float16\"\n",
        ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [\n",
        "    k\n",
        "    for k, v in globals().items()\n",
        "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
        "]\n",
        "config = {k: globals()[k] for k in config_keys}  # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ[\"RANK\"])\n",
        "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "    device = f\"cuda:{ddp_local_rank}\"\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank  # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"float16\": torch.float16,\n",
        "}[dtype]\n",
        "ctx = (\n",
        "    nullcontext()\n",
        "    if device_type == \"cpu\"\n",
        "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        ")\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join(data_folder, dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if device_type == \"cuda\":\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = (\n",
        "            x.pin_memory().to(device, non_blocking=True),\n",
        "            y.pin_memory().to(device, non_blocking=True),\n",
        "        )\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, \"meta.pkl\")\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta[\"vocab_size\"]\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    pos_enc=pos_enc,\n",
        "    vocab_size=meta_vocab_size,\n",
        "    dropout=dropout,\n",
        ")  # start with model_args from command line\n",
        "if init_from == \"scratch\":\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\n",
        "            \"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\"\n",
        "        )\n",
        "    model_args[\"vocab_size\"] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == \"resume\":\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(drive_path,out_dir, \"ckpt.pt\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint[\"model_args\"]\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint[\"model\"]\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint[\"iter_num\"]\n",
        "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
        "elif init_from.startswith(\"gpt2\"):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args[\n",
        "        \"block_size\"\n",
        "    ] = block_size  # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay, learning_rate, (beta1, beta2), device_type\n",
        ")\n",
        "if init_from == \"resume\":\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "checkpoint = None  # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model)  # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch(\"train\")  # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0  # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model  # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(\n",
        "            f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        if wandb_log:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train/loss\": losses[\"train\"],\n",
        "                    \"val/loss\": losses[\"val\"],\n",
        "                    \"lr\": lr,\n",
        "                    \"mfu\": running_mfu * 100,  # convert to percentage\n",
        "                }\n",
        "            )\n",
        "        if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses[\"val\"]\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    \"model\": raw_model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"model_args\": model_args,\n",
        "                    \"iter_num\": iter_num,\n",
        "                    \"best_val_loss\": best_val_loss,\n",
        "                    \"config\": config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(drive_path + out_dir, \"ckpt.pt\"))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (\n",
        "                micro_step == gradient_accumulation_steps - 1\n",
        "            )\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = (\n",
        "                loss / gradient_accumulation_steps\n",
        "            )  # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch(\"train\")\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5:  # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
        "        print(\n",
        "            f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\"\n",
        "        )\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I-H61zUnKhbX",
        "outputId": "ae05fbab-235a-4721-e3c0-1711e337b75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 21 (inside /content/drive/MyDrive/bai/data/multiplication/meta.pkl)\n",
            "Resuming training from out-multi\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 38, with 10,739,328 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjsanflo\u001b[0m (\u001b[33mbai_seminar_2324\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240118_130608-436hv5lt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bai_seminar_2324/multiplication-char/runs/436hv5lt' target=\"_blank\">relative_pos_enc_100k</a></strong> to <a href='https://wandb.ai/bai_seminar_2324/multiplication-char' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bai_seminar_2324/multiplication-char' target=\"_blank\">https://wandb.ai/bai_seminar_2324/multiplication-char</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bai_seminar_2324/multiplication-char/runs/436hv5lt' target=\"_blank\">https://wandb.ai/bai_seminar_2324/multiplication-char/runs/436hv5lt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 95250: train loss 0.4739, val loss 0.4769\n",
            "iter 95250: loss 0.4769, time 111617.55ms, mfu -100.00%\n",
            "iter 95300: loss 0.4754, time 386.14ms, mfu 0.96%\n",
            "iter 95350: loss 0.4724, time 382.71ms, mfu 0.97%\n",
            "iter 95400: loss 0.4770, time 387.14ms, mfu 0.97%\n",
            "iter 95450: loss 0.4764, time 390.01ms, mfu 0.96%\n",
            "step 95500: train loss 0.4742, val loss 0.4778\n",
            "iter 95500: loss 0.4725, time 18828.00ms, mfu 0.87%\n",
            "iter 95550: loss 0.4719, time 389.51ms, mfu 0.88%\n",
            "iter 95600: loss 0.4761, time 385.37ms, mfu 0.89%\n",
            "iter 95650: loss 0.4778, time 389.00ms, mfu 0.89%\n",
            "iter 95700: loss 0.4679, time 383.65ms, mfu 0.90%\n",
            "step 95750: train loss 0.4744, val loss 0.4777\n",
            "iter 95750: loss 0.4727, time 18885.60ms, mfu 0.81%\n",
            "iter 95800: loss 0.4742, time 389.27ms, mfu 0.83%\n",
            "iter 95850: loss 0.4723, time 388.19ms, mfu 0.84%\n",
            "iter 95900: loss 0.4659, time 388.46ms, mfu 0.85%\n",
            "iter 95950: loss 0.4777, time 388.40ms, mfu 0.86%\n",
            "step 96000: train loss 0.4738, val loss 0.4772\n",
            "iter 96000: loss 0.4762, time 18872.19ms, mfu 0.78%\n",
            "iter 96050: loss 0.4717, time 387.02ms, mfu 0.80%\n",
            "iter 96100: loss 0.4795, time 387.54ms, mfu 0.81%\n",
            "iter 96150: loss 0.4769, time 392.28ms, mfu 0.83%\n",
            "iter 96200: loss 0.4794, time 385.31ms, mfu 0.84%\n",
            "step 96250: train loss 0.4741, val loss 0.4776\n",
            "iter 96250: loss 0.4715, time 18812.12ms, mfu 0.76%\n",
            "iter 96300: loss 0.4771, time 388.51ms, mfu 0.78%\n",
            "iter 96350: loss 0.4719, time 391.22ms, mfu 0.80%\n",
            "iter 96400: loss 0.4754, time 391.45ms, mfu 0.81%\n",
            "iter 96450: loss 0.4753, time 384.94ms, mfu 0.83%\n",
            "step 96500: train loss 0.4743, val loss 0.4776\n",
            "iter 96500: loss 0.4731, time 18830.26ms, mfu 0.75%\n",
            "iter 96550: loss 0.4758, time 386.88ms, mfu 0.77%\n",
            "iter 96600: loss 0.4727, time 393.83ms, mfu 0.79%\n",
            "iter 96650: loss 0.4740, time 384.99ms, mfu 0.80%\n",
            "iter 96700: loss 0.4748, time 384.79ms, mfu 0.82%\n",
            "step 96750: train loss 0.4740, val loss 0.4769\n",
            "iter 96750: loss 0.4772, time 18879.65ms, mfu 0.74%\n",
            "iter 96800: loss 0.4704, time 385.68ms, mfu 0.76%\n",
            "iter 96850: loss 0.4774, time 389.71ms, mfu 0.78%\n",
            "iter 96900: loss 0.4741, time 398.46ms, mfu 0.80%\n",
            "iter 96950: loss 0.4757, time 389.36ms, mfu 0.81%\n",
            "step 97000: train loss 0.4741, val loss 0.4774\n",
            "iter 97000: loss 0.4729, time 18837.12ms, mfu 0.73%\n",
            "iter 97050: loss 0.4757, time 385.96ms, mfu 0.76%\n",
            "iter 97100: loss 0.4742, time 384.36ms, mfu 0.78%\n",
            "iter 97150: loss 0.4736, time 387.93ms, mfu 0.80%\n",
            "iter 97200: loss 0.4731, time 387.02ms, mfu 0.81%\n",
            "step 97250: train loss 0.4739, val loss 0.4776\n",
            "iter 97250: loss 0.4756, time 18852.76ms, mfu 0.73%\n",
            "iter 97300: loss 0.4726, time 384.26ms, mfu 0.76%\n",
            "iter 97350: loss 0.4725, time 386.72ms, mfu 0.78%\n",
            "iter 97400: loss 0.4736, time 391.71ms, mfu 0.80%\n",
            "iter 97450: loss 0.4752, time 385.29ms, mfu 0.81%\n",
            "step 97500: train loss 0.4742, val loss 0.4778\n",
            "iter 97500: loss 0.4740, time 18880.16ms, mfu 0.73%\n",
            "iter 97550: loss 0.4720, time 382.47ms, mfu 0.76%\n",
            "iter 97600: loss 0.4738, time 386.45ms, mfu 0.78%\n",
            "iter 97650: loss 0.4731, time 387.57ms, mfu 0.80%\n",
            "iter 97700: loss 0.4742, time 396.60ms, mfu 0.81%\n",
            "step 97750: train loss 0.4742, val loss 0.4776\n",
            "iter 97750: loss 0.4714, time 18819.68ms, mfu 0.73%\n",
            "iter 97800: loss 0.4720, time 391.16ms, mfu 0.75%\n",
            "iter 97850: loss 0.4718, time 388.93ms, mfu 0.77%\n",
            "iter 97900: loss 0.4744, time 385.93ms, mfu 0.79%\n",
            "iter 97950: loss 0.4729, time 387.59ms, mfu 0.81%\n",
            "step 98000: train loss 0.4743, val loss 0.4775\n",
            "iter 98000: loss 0.4743, time 18905.01ms, mfu 0.73%\n",
            "iter 98050: loss 0.4717, time 390.29ms, mfu 0.75%\n",
            "iter 98100: loss 0.4733, time 388.61ms, mfu 0.77%\n",
            "iter 98150: loss 0.4716, time 389.48ms, mfu 0.79%\n",
            "iter 98200: loss 0.4736, time 392.15ms, mfu 0.81%\n",
            "step 98250: train loss 0.4744, val loss 0.4773\n",
            "iter 98250: loss 0.4724, time 18859.79ms, mfu 0.73%\n",
            "iter 98300: loss 0.4767, time 390.35ms, mfu 0.75%\n",
            "iter 98350: loss 0.4750, time 384.97ms, mfu 0.77%\n",
            "iter 98400: loss 0.4716, time 388.14ms, mfu 0.79%\n",
            "iter 98450: loss 0.4724, time 387.71ms, mfu 0.81%\n",
            "step 98500: train loss 0.4739, val loss 0.4774\n",
            "iter 98500: loss 0.4733, time 18821.46ms, mfu 0.73%\n",
            "iter 98550: loss 0.4745, time 386.23ms, mfu 0.75%\n",
            "iter 98600: loss 0.4720, time 390.05ms, mfu 0.77%\n",
            "iter 98650: loss 0.4770, time 389.30ms, mfu 0.79%\n",
            "iter 98700: loss 0.4697, time 387.55ms, mfu 0.81%\n",
            "step 98750: train loss 0.4740, val loss 0.4772\n",
            "iter 98750: loss 0.4765, time 18893.63ms, mfu 0.73%\n",
            "iter 98800: loss 0.4741, time 385.70ms, mfu 0.75%\n",
            "iter 98850: loss 0.4703, time 388.63ms, mfu 0.77%\n",
            "iter 98900: loss 0.4717, time 388.40ms, mfu 0.79%\n",
            "iter 98950: loss 0.4762, time 390.98ms, mfu 0.81%\n",
            "step 99000: train loss 0.4742, val loss 0.4772\n",
            "iter 99000: loss 0.4750, time 19239.14ms, mfu 0.73%\n",
            "iter 99050: loss 0.4719, time 391.09ms, mfu 0.75%\n",
            "iter 99100: loss 0.4690, time 386.77ms, mfu 0.77%\n",
            "iter 99150: loss 0.4747, time 389.52ms, mfu 0.79%\n",
            "iter 99200: loss 0.4770, time 388.53ms, mfu 0.81%\n",
            "step 99250: train loss 0.4740, val loss 0.4775\n",
            "iter 99250: loss 0.4731, time 18848.72ms, mfu 0.73%\n",
            "iter 99300: loss 0.4711, time 386.11ms, mfu 0.75%\n",
            "iter 99350: loss 0.4759, time 387.29ms, mfu 0.77%\n",
            "iter 99400: loss 0.4756, time 386.36ms, mfu 0.79%\n",
            "iter 99450: loss 0.4716, time 388.22ms, mfu 0.81%\n",
            "step 99500: train loss 0.4742, val loss 0.4779\n",
            "iter 99500: loss 0.4776, time 19001.05ms, mfu 0.73%\n",
            "iter 99550: loss 0.4725, time 391.42ms, mfu 0.75%\n",
            "iter 99600: loss 0.4728, time 387.50ms, mfu 0.77%\n",
            "iter 99650: loss 0.4748, time 387.57ms, mfu 0.79%\n",
            "iter 99700: loss 0.4683, time 389.26ms, mfu 0.81%\n",
            "step 99750: train loss 0.4746, val loss 0.4780\n",
            "iter 99750: loss 0.4767, time 18845.02ms, mfu 0.73%\n",
            "iter 99800: loss 0.4760, time 389.57ms, mfu 0.75%\n",
            "iter 99850: loss 0.4722, time 391.38ms, mfu 0.77%\n",
            "iter 99900: loss 0.4735, time 386.41ms, mfu 0.79%\n",
            "iter 99950: loss 0.4747, time 389.29ms, mfu 0.81%\n",
            "step 100000: train loss 0.4744, val loss 0.4777\n",
            "iter 100000: loss 0.4757, time 18846.28ms, mfu 0.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "3r0kvdIfuCu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Sample from a trained model\n",
        "\"\"\"\n",
        "data_folder = drive_path + \"/data\"\n",
        "\n",
        "def sample(data_file_name=\"numbers.txt\"):\n",
        "    # -----------------------------------------------------------------------------\n",
        "    init_from = (\n",
        "        \"resume\"  # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "    )\n",
        "    start = \"\"\n",
        "    drive_path = \"/content/drive/MyDrive/bai/\"\n",
        "    data_folder = drive_path + \"data\"\n",
        "    out_dir = drive_path + \"out-multi\"\n",
        "    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "    print(f\"checkpoint from: {ckpt_path}\")\n",
        "    num_samples = 1  # number of samples to draw\n",
        "    max_new_tokens = 8 # number of tokens generated in each sample\n",
        "    temperature = (\n",
        "        0.1  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "    )\n",
        "    top_k = 20  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "    seed = 1337\n",
        "    device = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "    dtype = (\n",
        "        \"bfloat16\"\n",
        "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "        else \"float16\"\n",
        "    )  # 'float32' or 'bfloat16' or 'float16'\n",
        "    compile = False  # use PyTorch 2.0 to compile the model to be faster\n",
        "    #exec(open(\"configurator.py\").read())  # overrides from command line or config file\n",
        "    # -----------------------------------------------------------------------------\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
        "    torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
        "    device_type = (\n",
        "        \"cuda\" if \"cuda\" in device else \"cpu\"\n",
        "    )  # for later use in torch.autocast\n",
        "    ptdtype = {\n",
        "        \"float32\": torch.float32,\n",
        "        \"bfloat16\": torch.bfloat16,\n",
        "        \"float16\": torch.float16,\n",
        "    }[dtype]\n",
        "    ctx = (\n",
        "        nullcontext()\n",
        "        if device_type == \"cpu\"\n",
        "        else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "    )\n",
        "\n",
        "    # model\n",
        "    if init_from == \"resume\":\n",
        "        # init from a model saved in a specific directory\n",
        "        ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "        gptconf = GPTConfig(**checkpoint[\"model_args\"])\n",
        "        model = GPT(gptconf)\n",
        "        state_dict = checkpoint[\"model\"]\n",
        "        unwanted_prefix = \"_orig_mod.\"\n",
        "        for k, v in list(state_dict.items()):\n",
        "            if k.startswith(unwanted_prefix):\n",
        "                state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "        model.load_state_dict(state_dict)\n",
        "    elif init_from.startswith(\"gpt2\"):\n",
        "        # init from a given GPT-2 model\n",
        "        model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    if compile:\n",
        "        model = torch.compile(model)  # requires PyTorch 2.0 (optional)\n",
        "\n",
        "    # look for the meta pickle in case it is available in the dataset folder\n",
        "    load_meta = False\n",
        "\n",
        "    if (\n",
        "        init_from == \"resume\"\n",
        "        and \"config\" in checkpoint\n",
        "        and \"dataset\" in checkpoint[\"config\"]\n",
        "    ):  # older checkpoints might not have these...\n",
        "        meta_path = os.path.join(\n",
        "            data_folder, checkpoint[\"config\"][\"dataset\"], \"meta.pkl\"\n",
        "        )\n",
        "        load_meta = os.path.exists(meta_path)\n",
        "        #print(\"meta_path:\", meta_path)\n",
        "    if load_meta:\n",
        "        print(f\"Loading meta from {meta_path}...\")\n",
        "        with open(meta_path, \"rb\") as f:\n",
        "            meta = pickle.load(f)\n",
        "        # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
        "        stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "        encode = lambda s: [stoi[c] for c in s]\n",
        "        decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "    else:\n",
        "        # ok let's assume gpt-2 encodings by default\n",
        "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "        decode = lambda l: enc.decode(l)\n",
        "\n",
        "    # encode the beginning of the prompt\n",
        "    if start.startswith(\"FILE:\"):\n",
        "        with open(start[5:], \"r\", encoding=\"utf-8\") as f:\n",
        "            start = f.read()\n",
        "\n",
        "    acc = 0\n",
        "    per_digit_acc = {\"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0, \"6\": 0, \"7\": 0}\n",
        "    # Initialize a dictionary to store character-wise accuracy\n",
        "    per_sample_digit_length_distance = {}\n",
        "\n",
        "    eval_list = create_eval_list(data_file_name)[:10000]\n",
        "    pairs = []\n",
        "    print(f\"len(eval_list): {len(eval_list)}\")\n",
        "\n",
        "    for eval_sample in eval_list:\n",
        "        start_ids = encode(eval_sample[0])\n",
        "        x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with ctx:\n",
        "                for k in range(num_samples):\n",
        "                    y = model.generate(\n",
        "                        x, max_new_tokens, temperature=temperature, top_k=top_k\n",
        "                    )\n",
        "\n",
        "                    pred = decode(y[0].tolist()).split(\"\\n\")\n",
        "                    if len(pred) > 1:\n",
        "                        pred = pred[1]\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    pairs.append((pred, eval_sample[1]))\n",
        "\n",
        "                    # Calculate accuracy per digit\n",
        "                    if pred == eval_sample[1]:\n",
        "                        acc += 1\n",
        "                        for key in per_digit_acc:\n",
        "                            per_digit_acc[key] += 1\n",
        "\n",
        "                    else:\n",
        "                        #print(eval_sample[1], pred[0])\n",
        "                        for index, char in enumerate(eval_sample[1]):\n",
        "                            if 0 <= index < len(pred) and char == pred[index]:\n",
        "                                per_digit_acc[str(index + 1)] += 1\n",
        "\n",
        "                    # Calculate accuracy distance per sample digit length\n",
        "                    sample_digit_length = len(eval_sample[1])\n",
        "                    try:\n",
        "                      distance = abs(int(pred.replace(\" \", \"\")) - int(eval_sample[1].replace(\" \", \"\")))\n",
        "                      if sample_digit_length not in per_sample_digit_length_distance:\n",
        "                          per_sample_digit_length_distance[sample_digit_length] = {}\n",
        "                      if distance not in per_sample_digit_length_distance[sample_digit_length]:\n",
        "                          per_sample_digit_length_distance[sample_digit_length][distance] = 1\n",
        "                      else:\n",
        "                          per_sample_digit_length_distance[sample_digit_length][distance] += 1\n",
        "                    except ValueError:\n",
        "                      # Handle the case where conversion to int fails\n",
        "                      distance = None\n",
        "\n",
        "    print(\"acc:\", acc)\n",
        "    for key in per_digit_acc:\n",
        "        per_digit_acc[key] = per_digit_acc[key] / len(eval_list)\n",
        "\n",
        "    print(f\"General Accuracy: {acc/len(eval_list)}\\n\\n\")\n",
        "\n",
        "    print(f\"Accuracy per digit position: {acc/len(eval_list)}\\n\\n\")\n",
        "    for key, value in per_digit_acc.items():\n",
        "        print(f\"{key}\\t{value}\")\n",
        "\n",
        "    # Print accuracy distance per sample digit length\n",
        "    print(\"\\nAccuracy Distance per Sample Digit Length:\")\n",
        "    for digit_length, distance_info in sorted(per_sample_digit_length_distance.items()):\n",
        "        print(f\"\\n- Samples of digit length {digit_length}:\")\n",
        "\n",
        "        # Sort distances by occurrences in descending order and take the first 10\n",
        "        sorted_distances = sorted(distance_info.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "        for distance, occurrences in sorted_distances:\n",
        "            print(f\"  - Distance {distance}: {occurrences}\")\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def create_eval_list(data_file_name) -> list[tuple[str, str]]:\n",
        "    data_file_name = data_file_name\n",
        "    eval = []\n",
        "    # replace with current number test file path\n",
        "    print(f\"Evaluation list: {os.path.join(data_folder, dataset, data_file_name)}\")\n",
        "    with open(\n",
        "        os.path.join(data_folder, dataset, data_file_name),\n",
        "        \"r\",\n",
        "    ) as file:\n",
        "        while True:\n",
        "            line1 = file.readline()\n",
        "            line2 = file.readline()\n",
        "            file.readline()\n",
        "            if not line2:\n",
        "                break\n",
        "            else:\n",
        "                eval.append((line1.strip(), line2.strip()))\n",
        "    return eval"
      ],
      "metadata": {
        "id": "re21Ju0Q3ZgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = sample(data_file_name=\"numbers.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRfZ_uf26UFy",
        "outputId": "fbc6ec67-183a-479c-9d87-551fca21ed7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint from: /content/drive/MyDrive/bai/out-multi/ckpt.pt\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 10.65M\n",
            "Loading meta from /content/drive/MyDrive/bai/data/multiplication/meta.pkl...\n",
            "Evaluation list: /content/drive/MyDrive/bai//data/multiplication/numbers.txt\n",
            "len(eval_list): 10000\n",
            "acc: 9787\n",
            "General Accuracy: 0.9787\n",
            "\n",
            "\n",
            "Accuracy per digit position: 0.9787\n",
            "\n",
            "\n",
            "1\t0.9978\n",
            "2\t0.9859\n",
            "3\t0.9855\n",
            "4\t0.991\n",
            "5\t0.9997\n",
            "6\t0.9925\n",
            "7\t0.9787\n",
            "\n",
            "Accuracy Distance per Sample Digit Length:\n",
            "\n",
            "- Samples of digit length 3:\n",
            "  - Distance 0: 16\n",
            "  - Distance 60: 1\n",
            "\n",
            "- Samples of digit length 4:\n",
            "  - Distance 0: 66\n",
            "\n",
            "- Samples of digit length 5:\n",
            "  - Distance 0: 1666\n",
            "  - Distance 200: 33\n",
            "  - Distance 1000: 23\n",
            "  - Distance 100: 11\n",
            "  - Distance 800: 2\n",
            "  - Distance 10000: 2\n",
            "  - Distance 2000: 1\n",
            "  - Distance 400: 1\n",
            "\n",
            "- Samples of digit length 6:\n",
            "  - Distance 0: 8039\n",
            "  - Distance 10000: 40\n",
            "  - Distance 800: 35\n",
            "  - Distance 400: 28\n",
            "  - Distance 200: 18\n",
            "  - Distance 1000: 7\n",
            "  - Distance 100: 6\n",
            "  - Distance 2000: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = sample(data_file_name=\"numbers_test.txt\")"
      ],
      "metadata": {
        "id": "KmVOvizo7sQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1c6c3b-c65e-442f-e42a-d594ba7655c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint from: /content/drive/MyDrive/bai/out-multi/ckpt.pt\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 10.65M\n",
            "Loading meta from /content/drive/MyDrive/bai/data/multiplication/meta.pkl...\n",
            "Evaluation list: /content/drive/MyDrive/bai//data/multiplication/numbers_test.txt\n",
            "len(eval_list): 10000\n",
            "acc: 9409\n",
            "General Accuracy: 0.9409\n",
            "\n",
            "\n",
            "Accuracy per digit position: 0.9409\n",
            "\n",
            "\n",
            "1\t0.9945\n",
            "2\t0.97\n",
            "3\t0.9563\n",
            "4\t0.97\n",
            "5\t0.9979\n",
            "6\t0.9812\n",
            "7\t0.9409\n",
            "\n",
            "Accuracy Distance per Sample Digit Length:\n",
            "\n",
            "- Samples of digit length 1:\n",
            "  - Distance 2: 1\n",
            "\n",
            "- Samples of digit length 3:\n",
            "  - Distance 0: 12\n",
            "  - Distance 10: 5\n",
            "  - Distance 20: 1\n",
            "\n",
            "- Samples of digit length 4:\n",
            "  - Distance 0: 80\n",
            "  - Distance 100: 8\n",
            "\n",
            "- Samples of digit length 5:\n",
            "  - Distance 0: 1563\n",
            "  - Distance 200: 70\n",
            "  - Distance 1000: 65\n",
            "  - Distance 100: 28\n",
            "  - Distance 800: 3\n",
            "  - Distance 2000: 3\n",
            "  - Distance 400: 1\n",
            "  - Distance 10000: 1\n",
            "\n",
            "- Samples of digit length 6:\n",
            "  - Distance 0: 7754\n",
            "  - Distance 800: 118\n",
            "  - Distance 400: 98\n",
            "  - Distance 10000: 69\n",
            "  - Distance 200: 50\n",
            "  - Distance 1000: 40\n",
            "  - Distance 100: 11\n",
            "  - Distance 2000: 6\n",
            "  - Distance 1200: 6\n",
            "  - Distance 9900: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = sample(data_file_name=\"ood_numbers.txt\")"
      ],
      "metadata": {
        "id": "RnjKbXif7tXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82cd5ed3-56fd-423f-a562-aff20b62903e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint from: /content/drive/MyDrive/bai/out-multi/ckpt.pt\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "Using relative positional encoding!\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 10.65M\n",
            "Loading meta from /content/drive/MyDrive/bai/data/multiplication/meta.pkl...\n",
            "Evaluation list: /content/drive/MyDrive/bai//data/multiplication/ood_numbers.txt\n",
            "len(eval_list): 10000\n",
            "acc: 0\n",
            "General Accuracy: 0.0\n",
            "\n",
            "\n",
            "Accuracy per digit position: 0.0\n",
            "\n",
            "\n",
            "1\t0.5667\n",
            "2\t0.1583\n",
            "3\t0.096\n",
            "4\t0.1082\n",
            "5\t0.1309\n",
            "6\t0.1253\n",
            "7\t0.0\n",
            "\n",
            "Accuracy Distance per Sample Digit Length:\n",
            "\n",
            "- Samples of digit length 6:\n",
            "  - Distance 720000: 7\n",
            "  - Distance 607200: 4\n",
            "  - Distance 796000: 4\n",
            "  - Distance 504000: 4\n",
            "  - Distance 792000: 4\n",
            "  - Distance 571200: 4\n",
            "  - Distance 680400: 3\n",
            "  - Distance 843400: 3\n",
            "  - Distance 487800: 3\n",
            "  - Distance 293900: 3\n",
            "\n",
            "- Samples of digit length 7:\n",
            "  - Distance 1026000: 5\n",
            "  - Distance 1246000: 5\n",
            "  - Distance 1260000: 5\n",
            "  - Distance 2376000: 5\n",
            "  - Distance 2352000: 5\n",
            "  - Distance 5236000: 4\n",
            "  - Distance 1530000: 4\n",
            "  - Distance 1478400: 4\n",
            "  - Distance 1190400: 4\n",
            "  - Distance 1670000: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "id": "OIVMe9JDuG5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(data):\n",
        "  data = [x for x in data if x[0] != x[1]]\n",
        "  print(data[0])\n",
        "  actual = sorted([int(val[1]) for val in data])\n",
        "  predicted = sorted([int(val[0]) for val in data])\n",
        "  plt.plot(actual,np.arange(len(data)),color = \"blue\")\n",
        "  plt.plot(predicted,np.arange(len(data)),color = \"red\")\n",
        "  ticks = np.arange(0,max(predicted), 100000)\n",
        "  #print(max(predicted))\n",
        "  #print(len(ticks))\n",
        "  plt.xticks(ticks, rotation=45)\n",
        "\n",
        "  plt.title('Sample of unequal values for 3 x 3 down to 1 x 1 multiplications')\n",
        "  plt.xlabel('Index')\n",
        "  plt.ylabel('Multiplication results')\n",
        "  plt.legend()\n",
        "\n",
        "  #plt.savefig(\"figure.png\")\n",
        "  plt.show()\n",
        "\n",
        "plot(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "I81tWFmpFylK",
        "outputId": "3a4cf2bd-df74-4001-e30d-051eacff7192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('59060', '60060')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}